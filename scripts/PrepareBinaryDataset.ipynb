{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(data_dir, filename):\n",
    "    \"\"\"\n",
    "    Loads csv files where column 0 represents labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(data_dir, filename), header=None)\n",
    "    df2 = np.array(df)\n",
    "    labels = df2[:,0]\n",
    "    df2 = df2[:,1:]\n",
    "    return df2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBinary(data, labels, classLabel):\n",
    "    \"\"\"\n",
    "    Makes the dataset binary by changing the given class label to 1. \n",
    "    \"\"\"\n",
    "    zippedData = list(zip(data, labels))\n",
    "    zippedData = [[dataPoint, 1] if label == 8 else [dataPoint, 0] for dataPoint, label in zippedData]\n",
    "    data, labels = list(zip(*zippedData))\n",
    "    data, labels = np.array(data), np.array(labels)\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    print(data.shape, labels.shape)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBinaryBalanced(data, labels, posClassLabel, negClassLabel):\n",
    "    \"\"\"\n",
    "    Makes the dataset binary and balanced by changing the posClassLabel class to 0, and the negClassLabel class to 1.\n",
    "    \"\"\"\n",
    "    zippedData = list(zip(data, labels))\n",
    "    zippedData = [[dataPoint, 1] if label == posClassLabel else [dataPoint, 0] for dataPoint, label in zippedData \n",
    "                  if label in [posClassLabel, negClassLabel]]\n",
    "    \n",
    "    data, labels = list(zip(*zippedData))\n",
    "    data, labels = np.array(data), np.array(labels)\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    print(data.shape, labels.shape)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_padded(matrix,labels, n):\n",
    "    a = np.arange(matrix.shape[1])\n",
    "    padding = (-len(a))%n\n",
    "    index_arrays = np.split(np.concatenate((a,np.zeros(padding))).astype(int),n)\n",
    "    index_arrays[-1] = np.trim_zeros(index_arrays[-1] , \"b\")\n",
    "    matrices = [np.hstack((labels[:,np.newaxis], matrix[:, index_arrays[i]])) for i in range(len(index_arrays))]\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalPartition(data, labels):\n",
    "    assert round(sum(probVector),3) == 1\n",
    "    df = copy(data)\n",
    "    numFeatures = data.shape[1]\n",
    "    splitDfs = []\n",
    "    for i in range(len(probVector)):\n",
    "        numFeats = probVector[i]*numFeatures\n",
    "        tempDf = df[:,0:min(int(numFeats), df.shape[1]-1)]\n",
    "        tempDf = np.hstack((labels[:,np.newaxis], tempDf))\n",
    "        splitDfs.append(tempDf)\n",
    "        df = df[:, int(numFeats)+1:]\n",
    "    return splitDfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verticalPartition(data, labels, probVector):\n",
    "    \"\"\"\n",
    "    Vertically partitions the dataset by dividing the features according to probVector. \n",
    "    probVector must add to 1, as we don't consider overlapping partitions as of now.\n",
    "    \"\"\"\n",
    "    assert round(sum(probVector),3) == 1\n",
    "    df = copy(data)\n",
    "    numFeatures = data.shape[1]\n",
    "    splitDfs = []\n",
    "    for i in range(len(probVector)):\n",
    "        numFeats = probVector[i]*numFeatures\n",
    "        tempDf = df[:,0:min(int(numFeats), df.shape[1]-1)]\n",
    "        tempDf = np.hstack((labels[:,np.newaxis], tempDf))\n",
    "        splitDfs.append(tempDf)\n",
    "        df = df[:, int(numFeats)+1:]\n",
    "    return splitDfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSplitFiles(data_dir, baseFilename, splitDfs):\n",
    "    \"\"\"\n",
    "    Saves files into respective CSV files.\n",
    "    \"\"\"\n",
    "    for i in range(len(splitDfs)):\n",
    "        temp_df = splitDfs[i]\n",
    "        temp_filename = baseFilename.split(\".\")[0] + \"_\" + str(i) + \".csv\"        \n",
    "        temp_df = pd.DataFrame(data=temp_df, index=None)\n",
    "        temp_df.to_csv(os.path.join(data_dir, temp_filename), index=False, header=False)\n",
    "        print(\"File saved in {}\".format(os.path.join(data_dir, temp_filename)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11982, 784) (11982,)\n",
      "[(11982, 80), (11982, 80), (11982, 80), (11982, 80), (11982, 80), (11982, 80), (11982, 80), (11982, 80), (11982, 80), (11982, 74)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_train_9.csv\n",
      "Counter({1: 6131, 0: 5851})\n",
      "(11982, 785)\n",
      "(1984, 784) (1984,)\n",
      "[(1984, 80), (1984, 80), (1984, 80), (1984, 80), (1984, 80), (1984, 80), (1984, 80), (1984, 80), (1984, 80), (1984, 74)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnistbalanced\\mnistbalanced_test_9.csv\n",
      "Counter({1: 1010, 0: 974})\n",
      "(1984, 785)\n"
     ]
    }
   ],
   "source": [
    "#MNIST BALANCED\n",
    "# Process training set\n",
    "data_dir = \"../dl4j-examples/dl4j-examples/data/mnistbalanced\"\n",
    "trainFilename = \"mnistbalanced_train.csv\"\n",
    "testFilename = \"mnistbalanced_test.csv\"\n",
    "data, labels = loadData(data_dir, trainFilename)\n",
    "data, labels = makeBinaryBalanced(data, labels, 3, 8)\n",
    "data = data/255.\n",
    "splitDfs = split_padded(data, labels, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "\n",
    "df_to_save = pd.DataFrame(np.hstack((labels.reshape(-1,1), data)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, trainFilename.split(\".\")[0]+ \"_binary.csv\"), index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process test set\n",
    "data, labels = loadData(data_dir, testFilename)\n",
    "data, labels = makeBinaryBalanced(data, labels, 3, 8)\n",
    "data = data/255.\n",
    "splitDfs = split_padded(data, labels, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "np.min(data), np.max(data)\n",
    "\n",
    "\n",
    "# Save all\n",
    "\n",
    "df_to_save = pd.DataFrame(np.hstack((labels.reshape(-1,1), data)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, testFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "[(60000, 80), (60000, 80), (60000, 80), (60000, 80), (60000, 80), (60000, 80), (60000, 80), (60000, 80), (60000, 80), (60000, 74)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_train_9.csv\n",
      "Counter({0: 54149, 1: 5851})\n",
      "(60000, 785)\n",
      "(10000, 784) (10000,)\n",
      "[(10000, 80), (10000, 80), (10000, 80), (10000, 80), (10000, 80), (10000, 80), (10000, 80), (10000, 80), (10000, 80), (10000, 74)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/mnist\\mnist_test_9.csv\n",
      "Counter({0: 9026, 1: 974})\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#MNIST\n",
    "# Process training set\n",
    "\n",
    "data_dir = \"../dl4j-examples/dl4j-examples/data/mnist\"\n",
    "trainFilename = \"mnist_train.csv\"\n",
    "testFilename = \"mnist_test.csv\"\n",
    "numSplits = 10\n",
    "\n",
    "data, labels = loadData(data_dir, trainFilename)\n",
    "data, labels = makeBinary(data, labels, 8)\n",
    "data = data/255.\n",
    "#splitDfs = verticalPartition(data, labels, [1/numSplits]*numSplits)\n",
    "splitDfs = split_padded(data, labels, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "\n",
    "df_to_save = pd.DataFrame(np.hstack((labels.reshape(-1,1), data)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, trainFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Process test set\n",
    "data, labels = loadData(data_dir, testFilename)\n",
    "data, labels = makeBinary(data, labels, 8)\n",
    "data = data/255.\n",
    "#splitDfs = verticalPartition(data, labels, [1/numSplits]*numSplits)\n",
    "splitDfs = split_padded(data, labels, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "np.min(data), np.max(data)\n",
    "\n",
    "\n",
    "# Save all\n",
    "\n",
    "df_to_save = pd.DataFrame(np.hstack((labels.reshape(-1,1), data)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, testFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we deal with CIFAR-10\n",
    "data_dir = \"../data/cifar-10/\"\n",
    "cifar10TrainFileName = \"cifar10_train.csv\"\n",
    "cifar10TestFileName = \"cifar10_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCifar10(data_dir, dataset=\"train\"):\n",
    "    if dataset == \"train\":\n",
    "        files = [os.path.join(data_dir, \"data_batch_\" + str(i)) for i in range(1,6)]\n",
    "        data_dics = [unpickle(file) for file in files]\n",
    "        data = np.array([d[b'data'] for d in data_dics]).reshape(-1,3072)\n",
    "        labels = np.squeeze(np.array([d[b'labels'] for d in data_dics]).reshape(-1,1))\n",
    "    else:\n",
    "        file = os.path.join(data_dir, \"test_batch\")\n",
    "        data_dic = unpickle(file)\n",
    "        data = data_dic[b'data']\n",
    "        labels = np.squeeze(np.array(data_dic[b'labels']).reshape(-1,1))\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarData, cifarLabels = loadCifar10(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000,)\n",
      "[(50000, 309), (50000, 309), (50000, 309), (50000, 309), (50000, 309), (50000, 309), (50000, 309), (50000, 309), (50000, 309), (50000, 301)]\n",
      "File saved in ../data/cifar-10/cifar-10_train_0.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_1.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_2.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_3.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_4.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_5.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_6.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_7.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_8.csv\n",
      "File saved in ../data/cifar-10/cifar-10_train_9.csv\n",
      "Counter({0: 45000, 1: 5000})\n",
      "(50000, 3073)\n",
      "(10000, 3072) (10000,)\n",
      "[(10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 301)]\n",
      "File saved in ../data/cifar-10/cifar-10_test_0.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_1.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_2.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_3.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_4.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_5.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_6.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_7.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_8.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_9.csv\n",
      "Counter({0: 9000, 1: 1000})\n"
     ]
    }
   ],
   "source": [
    "#MNIST\n",
    "# Process training set\n",
    "\n",
    "data_dir = \"../data/cifar-10/\"\n",
    "trainFilename = \"cifar-10_train.csv\"\n",
    "testFilename = \"cifar-10_test.csv\"\n",
    "numSplits = 10\n",
    "\n",
    "cifarData, cifarLabels = loadCifar10(data_dir, \"train\")\n",
    "cifarData, cifarLabels = makeBinary(cifarData, cifarLabels, 8)\n",
    "cifarData = cifarData/255.\n",
    "#splitDfs = verticalPartition(data, labels, [1/numSplits]*numSplits)\n",
    "splitDfs = split_padded(cifarData, cifarLabels, numSplits)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(cifarLabels))\n",
    "\n",
    "df_to_save = pd.DataFrame(np.hstack((cifarLabels.reshape(-1,1), cifarData)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, trainFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n",
    "\n",
    "\n",
    "cifarTestData, cifarTestLabels = loadCifar10(data_dir, \"test\")\n",
    "cifarTestData, cifarTestLabels = makeBinary(cifarTestData, cifarTestLabels, 8)\n",
    "cifarTestData = cifarTestData/255.\n",
    "#splitDfs = verticalPartition(data, labels, [1/numSplits]*numSplits)\n",
    "splitDfs = split_padded(cifarTestData, cifarTestLabels, numSplits)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(cifarTestLabels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072) (10000,)\n",
      "[(10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 309), (10000, 301)]\n",
      "File saved in ../data/cifar-10/cifar-10_test_0.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_1.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_2.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_3.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_4.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_5.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_6.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_7.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_8.csv\n",
      "File saved in ../data/cifar-10/cifar-10_test_9.csv\n",
      "Counter({0: 9000, 1: 1000})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cifarTestlabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bafb1b4d9e55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifarTestLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf_to_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifarTestlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifarTestdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_to_save\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdf_to_save\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestFilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_binary.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cifarTestlabels' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "df_to_save = pd.DataFrame(np.hstack((cifarTestLabels.reshape(-1,1), cifarTestData)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, testFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1401, 200) (1401,)\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_train_9.csv\n",
      "Counter({0.0: 705, 1.0: 696})\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/synthetic\\synthetic_test_9.csv\n",
      "Counter({1.0: 301, 0.0: 300})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = \"../dl4j-examples/dl4j-examples/data/synthetic\"\n",
    "trainFilename = \"synthetic_train.csv\"\n",
    "testFilename = \"synthetic_test.csv\"\n",
    "numSplits = 10\n",
    "# Load and process synthetic data\n",
    "# Process training set\n",
    "data, labels = loadData(data_dir, trainFilename)\n",
    "print(data.shape, labels.shape)\n",
    "splitDfs = verticalPartition(data, labels, [1/numSplits]*numSplits)\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "\n",
    "\n",
    "\n",
    "# Process test set\n",
    "data, labels = loadData(data_dir, testFilename)\n",
    "splitDfs = verticalPartition(data, labels, [1/numSplits]*numSplits)\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 60) (166,)\n",
      "[(166, 7), (166, 7), (166, 7), (166, 7), (166, 7), (166, 7), (166, 7), (166, 7), (166, 7), (166, 7)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_train_9.csv\n",
      "Counter({1.0: 85, 0.0: 81})\n",
      "(166, 61)\n",
      "[(42, 7), (42, 7), (42, 7), (42, 7), (42, 7), (42, 7), (42, 7), (42, 7), (42, 7), (42, 7)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/sonar\\sonar_test_9.csv\n",
      "Counter({1.0: 26, 0.0: 16})\n",
      "(42, 61)\n"
     ]
    }
   ],
   "source": [
    "# Sonar data\n",
    "\n",
    "data_dir = \"../dl4j-examples/dl4j-examples/data/sonar\"\n",
    "trainFilename = \"sonar_train.csv\"\n",
    "testFilename = \"sonar_test.csv\"\n",
    "\n",
    "numSplits = 10\n",
    "# Load and process synthetic data\n",
    "# Process training set\n",
    "data, labels = loadData(data_dir, trainFilename)\n",
    "print(data.shape, labels.shape)\n",
    "splitDfs = split_padded(data, labels, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "df_to_save = pd.DataFrame(np.hstack((labels.reshape(-1,1), data)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, trainFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "# Process test set\n",
    "data, labels = loadData(data_dir, testFilename)\n",
    "splitDfs = split_padded(data, labels, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(labels))\n",
    "\n",
    "df_to_save = pd.DataFrame(np.hstack((labels.reshape(-1,1), data)))\n",
    "print(df_to_save.shape)\n",
    "df_to_save.to_csv(os.path.join(data_dir, testFilename.split(\".\")[0] + \"_binary.csv\"), index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datapackage\n",
    "\n",
    "data_url = 'https://datahub.io/machine-learning/madelon/datapackage.json'\n",
    "\n",
    "# to load Data Package into storage\n",
    "package = datapackage.Package(data_url)\n",
    "\n",
    "# to load only tabular data\n",
    "resources = package.resources\n",
    "for resource in resources:\n",
    "    if resource.tabular:\n",
    "        madelon_data = pd.read_csv(resource.descriptor['path'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.53658537, 0.48927039, ..., 0.46      , 0.69395018,\n",
       "        0.35263158],\n",
       "       [1.        , 0.48780488, 0.54077253, ..., 0.33      , 0.3772242 ,\n",
       "        0.4       ],\n",
       "       [0.        , 0.70731707, 0.43347639, ..., 0.39      , 0.70106762,\n",
       "        0.31578947],\n",
       "       [1.        , 0.48780488, 0.66094421, ..., 0.45      , 0.5088968 ,\n",
       "        0.59473684],\n",
       "       [1.        , 0.48780488, 0.57939914, ..., 0.45      , 0.52669039,\n",
       "        0.43684211]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_dir = \"../data/madelon/\"\n",
    "X_madelon, y_madelon = madelon_data.iloc[:, :-1], madelon_data.iloc[:, -1]\n",
    "X_madelon = np.array(X_madelon)\n",
    "y_madelon = np.array(y_madelon) \n",
    "X_madelon = MinMaxScaler().fit_transform(X_madelon) # scale\n",
    "y_madelon[y_madelon == 1] = 0\n",
    "y_madelon[y_madelon == 2] = 1\n",
    "X_madelon_train, X_madelon_test, y_madelon_train, y_madelon_test = train_test_split(X_madelon, y_madelon, test_size=0.2, random_state=42)\n",
    "\n",
    "all_train = np.hstack((y_madelon_train.reshape(-1,1), X_madelon_train))\n",
    "all_test = np.hstack((y_madelon_test.reshape(-1,1), X_madelon_test))\n",
    "pd.DataFrame(all_train).to_csv(os.path.join(data_dir, \"madelon_train_binary.csv\"), index=None, header=None)\n",
    "pd.DataFrame(all_test).to_csv(os.path.join(data_dir, \"madelon_test_binary.csv\"), index=None, header=None)\n",
    "all_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2080, 251), (2080, 251)]\n",
      "File saved in ../data/madelon\\madelon_train_0.csv\n",
      "File saved in ../data/madelon\\madelon_train_1.csv\n",
      "Counter({0: 1043, 1: 1037})\n",
      "[(520, 251), (520, 251)]\n",
      "File saved in ../data/madelon\\madelon_test_0.csv\n",
      "File saved in ../data/madelon\\madelon_test_1.csv\n",
      "Counter({1: 263, 0: 257})\n"
     ]
    }
   ],
   "source": [
    "numSplits = 2\n",
    "data_dir = \"../data/madelon\"\n",
    "trainFilename = \"madelon_train.csv\"\n",
    "testFilename = \"madelon_test.csv\"\n",
    "# Load and process synthetic data\n",
    "# Process training set\n",
    "splitDfs = split_padded(X_madelon_train, y_madelon_train, numSplits)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(y_madelon_train))\n",
    "\n",
    "\n",
    "# Process test set\n",
    "splitDfs = split_padded(X_madelon_test, y_madelon_test, numSplits)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(y_madelon_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../dl4j-examples/dl4j-examples/data/gisette/\"\n",
    "dataFilename = \"gisette_scale.csv\"\n",
    "gisette_data = pd.read_csv(os.path.join(data_dir, dataFilename))\n",
    "X_gisette, y_gisette = gisette_data.iloc[:, :-1], gisette_data.iloc[:, -1]\n",
    "X_gisette = np.array(X_gisette)\n",
    "y_gisette = np.array(y_gisette) \n",
    "y_gisette[y_gisette == -1] = 0\n",
    "y_gisette[y_gisette == 1] = 1\n",
    "\n",
    "X_gisette_train, X_gisette_test, y_gisette_train, y_gisette_test = train_test_split(X_gisette, y_gisette, test_size=0.2, random_state=42)\n",
    "all_train = np.hstack((y_gisette_train.reshape(-1,1), X_gisette_train))\n",
    "all_test = np.hstack((y_gisette_test.reshape(-1,1), X_gisette_test))\n",
    "pd.DataFrame(all_train).to_csv(os.path.join(data_dir, \"gisette_train_binary.csv\"), index=None, header=None)\n",
    "pd.DataFrame(all_test).to_csv(os.path.join(data_dir, \"gisette_test_binary.csv\"), index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4800, 501), (4800, 501), (4800, 501), (4800, 501), (4800, 501), (4800, 501), (4800, 501), (4800, 501), (4800, 501), (4800, 501)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_train_9.csv\n",
      "Counter({1: 2407, 0: 2393})\n",
      "[(1200, 501), (1200, 501), (1200, 501), (1200, 501), (1200, 501), (1200, 501), (1200, 501), (1200, 501), (1200, 501), (1200, 501)]\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_0.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_1.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_2.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_3.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_4.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_5.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_6.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_7.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_8.csv\n",
      "File saved in ../dl4j-examples/dl4j-examples/data/gisette\\gisette_test_9.csv\n",
      "Counter({0: 607, 1: 593})\n"
     ]
    }
   ],
   "source": [
    "numSplits = 10\n",
    "data_dir = \"../dl4j-examples/dl4j-examples/data/gisette\"\n",
    "trainFilename = \"gisette_train.csv\"\n",
    "testFilename = \"gisette_test.csv\"\n",
    "# Load and process synthetic data\n",
    "# Process training set\n",
    "splitDfs = split_padded(X_gisette_train, y_gisette_train, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(y_gisette_train))\n",
    "\n",
    "\n",
    "# Process test set\n",
    "splitDfs = split_padded(X_gisette_test, y_gisette_test, 10)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(y_gisette_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10001), (100, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "arcene_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.data', delimiter=\" \", header=None)\n",
    "arcene_labels = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.labels', delimiter=\" \", header=None)\n",
    "arcene_df.shape, arcene_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(80, 1001), (80, 1001), (80, 1001), (80, 1001), (80, 1001), (80, 1001), (80, 1001), (80, 1001), (80, 1001), (80, 1001)]\n",
      "File saved in ../data/arcene/arcene_train_0.csv\n",
      "File saved in ../data/arcene/arcene_train_1.csv\n",
      "File saved in ../data/arcene/arcene_train_2.csv\n",
      "File saved in ../data/arcene/arcene_train_3.csv\n",
      "File saved in ../data/arcene/arcene_train_4.csv\n",
      "File saved in ../data/arcene/arcene_train_5.csv\n",
      "File saved in ../data/arcene/arcene_train_6.csv\n",
      "File saved in ../data/arcene/arcene_train_7.csv\n",
      "File saved in ../data/arcene/arcene_train_8.csv\n",
      "File saved in ../data/arcene/arcene_train_9.csv\n",
      "Counter({0: 43, 1: 37})\n",
      "[(20, 1001), (20, 1001), (20, 1001), (20, 1001), (20, 1001), (20, 1001), (20, 1001), (20, 1001), (20, 1001), (20, 1001)]\n",
      "File saved in ../data/arcene/arcene_test_0.csv\n",
      "File saved in ../data/arcene/arcene_test_1.csv\n",
      "File saved in ../data/arcene/arcene_test_2.csv\n",
      "File saved in ../data/arcene/arcene_test_3.csv\n",
      "File saved in ../data/arcene/arcene_test_4.csv\n",
      "File saved in ../data/arcene/arcene_test_5.csv\n",
      "File saved in ../data/arcene/arcene_test_6.csv\n",
      "File saved in ../data/arcene/arcene_test_7.csv\n",
      "File saved in ../data/arcene/arcene_test_8.csv\n",
      "File saved in ../data/arcene/arcene_test_9.csv\n",
      "Counter({0: 13, 1: 7})\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/arcene/\"\n",
    "num_splits = 10\n",
    "trainFilename = \"arcene_train.csv\"\n",
    "testFilename = \"arcene_test.csv\"\n",
    "X_arcene, y_arcene = arcene_df.iloc[:, :-1], np.squeeze(arcene_labels)\n",
    "X_arcene = np.array(X_arcene)\n",
    "y_arcene = np.array(y_arcene) \n",
    "\n",
    "X_arcene = MinMaxScaler().fit_transform(X_arcene)\n",
    "y_arcene[y_arcene == -1] = 0\n",
    "y_arcene[y_arcene == 1] = 1\n",
    "\n",
    "X_arcene_train, X_arcene_test, y_arcene_train, y_arcene_test = train_test_split(X_arcene, y_arcene, test_size=0.2, random_state=42)\n",
    "all_train = np.hstack((y_arcene_train.reshape(-1,1), X_arcene_train))\n",
    "all_test = np.hstack((y_arcene_test.reshape(-1,1), X_arcene_test))\n",
    "\n",
    "# Process training set\n",
    "splitDfs = split_padded(X_arcene_train, y_arcene_train, num_splits)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, trainFilename, splitDfs)\n",
    "print(Counter(y_arcene_train))\n",
    "\n",
    "\n",
    "# Process test set\n",
    "splitDfs = split_padded(X_arcene_test, y_arcene_test, num_splits)\n",
    "print([a.shape for a in splitDfs])\n",
    "saveSplitFiles(data_dir, testFilename, splitDfs)\n",
    "print(Counter(y_arcene_test))\n",
    "\n",
    "pd.DataFrame(all_train).to_csv(os.path.join(data_dir, \"arcene_train_binary.csv\"), index=None, header=None)\n",
    "pd.DataFrame(all_test).to_csv(os.path.join(data_dir, \"arcene_test_binary.csv\"), index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 858: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6a7f6a7ec706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/dexter/dexter.arff'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\arff.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, encode_nominal, return_type)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArffDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m     return decoder.decode(fp, encode_nominal=encode_nominal,\n\u001b[1;32m-> 1059\u001b[1;33m                           return_type=return_type)\n\u001b[0m\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_nominal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDENSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\arff.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, encode_nominal, return_type)\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             return self._decode(s, encode_nominal=encode_nominal,\n\u001b[1;32m--> 892\u001b[1;33m                                 matrix_type=return_type)\n\u001b[0m\u001b[0;32m    893\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mArffException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m             \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_line\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\arff.py\u001b[0m in \u001b[0;36m_decode\u001b[1;34m(self, s, encode_nominal, matrix_type)\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mSTATE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TK_DESCRIPTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_line\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m             \u001b[1;31m# Ignore empty lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 858: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import arff, numpy as np\n",
    "dataset = arff.load(open('../data/dexter/dexter.arff'))\n",
    "data = np.array(dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
